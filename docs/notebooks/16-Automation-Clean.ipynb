{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating Machine Learning for Networking\n",
    "\n",
    "This notebook is designed to give you a **very** simple example of how to use [nPrint](https://nprint.github.io/nprint/) in a generic machine learning pipeline and the rapid pace at which we can train new models and test new ideas on network traffic. Note that this example is simply to show the pipeline, not to test a hard problem. The traffic collected is to the same website over the course of about 15 seconds.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this brief overview, you will use the nprint tool to generate fingerprints from packet captures (pcaps) that can be input to a variety of machine learning algorithms. By the end of this activity you will:\n",
    "\n",
    "1. Understand how to take a packet trace and represent it in a standard, generic form.\n",
    "2. Run nprint on a few classes of machine learning algorithms.\n",
    "3. Try the pipeline on some of your own (labeled) packet traces to understand how to build an end-to-end machine learning pipeline for a cybersecurity problem.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. Install nprint and run it on a packet trace---either the one provided, one that is available from public repositories (e.g., CICIDS). Optionally, try it on your own traffic.\n",
    "2. Run the notebook below to understand nprint and the basic machine learning pipeline.\n",
    "3. Try nprint on a new (security) classification problem, using one of the provided datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements\n",
    "\n",
    "nPrint must be installed into $PATH for external commands to work. Note: You may not be able to do this part in Google collab; it may only work if you have a local (Linux) machine on which you are running the notebook.  If that is the case, the second cell where you execute the commands on pcaps may not run, but we have provided the \"npt\" nprint output files as well, so you can run the rest of the notebook.\n",
    "\n",
    "You will want to install:\n",
    "\n",
    "* nPrint ([Installation instructions](https://github.com/nprint/nprint))\n",
    "* pcapML ([Installation instructions](https://github.com/nprint/pcapml))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generating nPrints from Traffic\n",
    "\n",
    "First, use nPrint to generate nPrints from each traffic trace, *only including the TCP headers in the nPrints*. (see which option you will need for that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd_80: Command not found.\n",
      "cmd_443: Command not found.\n"
     ]
    }
   ],
   "source": [
    "nprint = '/usr/local/bin/nprint'\n",
    "data = 'data/'\n",
    "\n",
    "cmd_http = '{} -P {}/http.pcap -t -W {}/http.npt'.format(nprint, data, data)\n",
    "cmd_log4j = '{} -P {}/log4j.pcap -t -W {}/log4j.npt'.format(nprint, data, data)\n",
    "!{cmd_80}\n",
    "!{cmd_443}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets examine the nPrints, which can be directly loaded with Pandas. Load the nprints using `read_csv` function in Pandas into data frames. How many packets are in each nprint? How many features are in each packet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP nPrint: Number of Packets: 24798, Features per packet: 240\n",
      "Log4j nPrint: Number of Packets: 80682, Features per packet: 240\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nprint_http = pd.read_csv('{}/http.npt'.format(data), index_col=0)\n",
    "nprint_log4j = pd.read_csv('{}/log4j.npt'.format(data), index_col=0)\n",
    "\n",
    "print('HTTP nPrint: Number of Packets: {0}, Features per packet: {1}'.format(nprint_http.shape[0], nprint_http.shape[1]))\n",
    "print('Log4j nPrint: Number of Packets: {0}, Features per packet: {1}'.format(nprint_log4j.shape[0], nprint_log4j.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that each nprint has the same number of features, which is the maximum number of bits in a TCP header. Let's look at the header itself.\n",
    "\n",
    "Notice how each bit (feature) is named according to the exact bit it represents in the packet, and all the possible bits of a TCP header are accounted for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['payload_bit_0', 'payload_bit_1', 'payload_bit_2', 'payload_bit_3',\n",
      "       'payload_bit_4', 'payload_bit_5', 'payload_bit_6', 'payload_bit_7',\n",
      "       'payload_bit_8', 'payload_bit_9',\n",
      "       ...\n",
      "       'payload_bit_230', 'payload_bit_231', 'payload_bit_232',\n",
      "       'payload_bit_233', 'payload_bit_234', 'payload_bit_235',\n",
      "       'payload_bit_236', 'payload_bit_237', 'payload_bit_238',\n",
      "       'payload_bit_239'],\n",
      "      dtype='object', length=240)\n",
      "Index(['payload_bit_0', 'payload_bit_1', 'payload_bit_2', 'payload_bit_3',\n",
      "       'payload_bit_4', 'payload_bit_5', 'payload_bit_6', 'payload_bit_7',\n",
      "       'payload_bit_8', 'payload_bit_9',\n",
      "       ...\n",
      "       'payload_bit_230', 'payload_bit_231', 'payload_bit_232',\n",
      "       'payload_bit_233', 'payload_bit_234', 'payload_bit_235',\n",
      "       'payload_bit_236', 'payload_bit_237', 'payload_bit_238',\n",
      "       'payload_bit_239'],\n",
      "      dtype='object', length=240)\n"
     ]
    }
   ],
   "source": [
    "print(nprint_http.columns)\n",
    "print(nprint_log4j.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: nPrint to Machine Learning Samples\n",
    "\n",
    "Now we need to take each nPrint and make each packet a \"sample\" for the machine learning task at hand. In this case, we'll set up a supervised learning task where port 80 traffic is labeled \"unencrypted\" and port 443 traffic is labeled \"encrypted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def label_data(data, label, features, labels):\n",
    "    for _, row in data.iterrows():\n",
    "        features.append(np.array(row))\n",
    "        labels.append(label)\n",
    "    return features, labels\n",
    "\n",
    "def train_eval(features,labels,clf):\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
    "\n",
    "    clf.fit(X_train, y_train) \n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Statistics\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(report)\n",
    "\n",
    "    # Let's also get the ROC AUC score while we're here, which requires a probability instead of just the prediction\n",
    "    y_pred_proba = clf.predict_proba(X_test)\n",
    "    # predict_proba gives us a probability estimate of each class, while roc_auc just cares about the \"positive\" class\n",
    "    y_pred_proba_pos = [sublist[1] for sublist in y_pred_proba]\n",
    "    roc = roc_auc_score(y_test, y_pred_proba_pos)\n",
    "    print('ROC AUC Score: {0}'.format(roc))\n",
    "\n",
    "def eval_nprint(class1, class2):\n",
    "    \n",
    "    (cmd1, label1) = class1\n",
    "    (cmd2, label2) = class2\n",
    "    \n",
    "    # Generate nPrints\n",
    "    !{cmd1}\n",
    "    !{cmd2}\n",
    "\n",
    "    # Load nPrints\n",
    "    df1 = pd.read_csv('{}/http.npt'.format(data), index_col=0)\n",
    "    df2 = pd.read_csv('{}/log4j.npt'.format(data), index_col=0)\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    (features, labels) = label_data(df1, label1, features, labels)\n",
    "    (features, labels) = label_data(df2, label2, features, labels)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    train_eval(features,labels,rf)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training a Classifier\n",
    "\n",
    "We're already ready to train and test a model on the traffic we gathered. Let's split the data into training and testing data, train a model, and get a stat report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Model\n",
    "\n",
    "nPrint's alignment of each packet allows for understanding the specific features (parts of the packet) that are driving the model's performance. It turns out that the options that are being set in the TCP header is actually more important than the port numbers themselves!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Exploring Different Representations\n",
    "\n",
    "Now that we have a generic pipeline, we can leverage nPrint's flags to generate different versions of nPrints. \n",
    "\n",
    "Test a version of this classification problem using **only** the IPv4 Headers of the packets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about testing using just the first 30 payload bytes in each packet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this representation performs less well. Why might that be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This hands-on demonstrated how nPrint can be used to rapidly train and test models for different traffic analysis problems. While this problem was contrived and simple, the same basic steps can be performed for any single-packet classification problem. \n",
    "\n",
    "If you want to train and test using **sets** of packets as input to a model, you'll either need a model that can handle that input, such as a CNN, or to flatten the 2D packet sample into a 1d sample for use with a model such as the random forest above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: pcapML\n",
    "\n",
    "pcapML is a system for improving the reproducability of traffic analysis tasks. pcapML leverages the pcapng file format to encode metadata directly into raw traffic captures, removing any ambiguity about which packets belong to any given traffic flow, application, attack, etc., while still being compatiable with popular tools such as tshark and tcpdump.\n",
    "\n",
    "For dataset curators, pcapML provides an easy way to encode metadata into raw traffic captures, ensuring the dataset is used in a consistent manner. On the analysis side, pcapML provides a standard dataset format for users to work with across different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "logging.getLogger(\"scapy\").setLevel(logging.CRITICAL)\n",
    "\n",
    "import pcapml_fe\n",
    "from pcapml_fe_helpers import *\n",
    "\n",
    "packets = []\n",
    "\n",
    "for traffic_sample in pcapml_fe.sampler('data/country-of-origin.pcapng'):\n",
    "    for pkt in traffic_sample.packets:\n",
    "        # Print packet timestamp and raw bytes\n",
    "        pip = IP(pkt.raw_bytes)\n",
    "        ptcp = TCP(pkt.raw_bytes)\n",
    "        packets.append((str(pip.src), ptcp.sport, str(pip.dst), ptcp.dport, len(pip), traffic_sample.metadata))\n",
    "            \n",
    "pdf = pd.DataFrame(packets, columns=['src IP', 'src port', 'dst IP', 'dst port', 'length', 'country'])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
