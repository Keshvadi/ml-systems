{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "approximate-venice",
   "metadata": {},
   "source": [
    "# Linear Regression on Network Traffic\n",
    "\n",
    "In this hands-on assignment, we will explore learn models, in particular linear regression. Linear models can work well when there is a linear relationship between the target variable being predicted and the input features. In other words, when the target prediction can be modeled as a linear combination of the input features, linear models may be appropriate.\n",
    "\n",
    "We'll explore the relationship beween bytes and packets in this hands-on, which may have a linear relationship at times. We will also explore how basis expansion---in particular polynomial basis expansion---can allow linear regression to fit more complex relationships between features and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa6ed7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"scapy.runtime\").setLevel(logging.ERROR)\n",
    "\n",
    "from netml.pparser.parser import PCAP\n",
    "from netml.utils.tool import dump_data, load_data\n",
    "\n",
    "# Plotting Library\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f81681f",
   "metadata": {},
   "source": [
    "## Group Traffic Info Flows \n",
    "\n",
    "This example below uses a packet capture that is provided in the repository. You are also welcome (and encouraged!) to capture your own traffic.\n",
    "\n",
    "Use `netml` to load the pcap and convert packets to flows, using pcap2flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e419519e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'_pcap2flows()' starts at 2022-10-27 10:47:35\n",
      "pcap_file: data/http.pcap\n",
      "ith_packets: 0\n",
      "ith_packets: 10000\n",
      "ith_packets: 20000\n",
      "len(flows): 593\n",
      "total number of flows: 593. Num of flows < 2 pkts: 300, and >=2 pkts: 293 without timeout splitting.\n",
      "kept flows: 293. Each of them has at least 2 pkts after timeout splitting.\n",
      "flow_durations.shape: (293, 1)\n",
      "        col_0\n",
      "count 293.000\n",
      "mean   11.629\n",
      "std    15.820\n",
      "min     0.000\n",
      "25%     0.076\n",
      "50%     0.455\n",
      "75%    20.097\n",
      "max    46.235\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 293 entries, 0 to 292\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   col_0   293 non-null    float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 2.4 KB\n",
      "None\n",
      "0th_flow: len(pkts): 4\n",
      "After splitting flows, the number of subflows: 291 and each of them has at least 2 packets.\n",
      "'_pcap2flows()' ends at 2022-10-27 10:47:40 and takes 0.0831 mins.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpcap = PCAP('data/http.pcap', flow_ptks_thres=2, verbose=10)\n",
    "hpcap.pcap2flows()\n",
    "len(hpcap.flows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d90feb",
   "metadata": {},
   "source": [
    "## Generate Features\n",
    "\n",
    "Use the `netml` \"STATS\" option to generate features for each flow, which will provide the following features for each flow:\n",
    "\n",
    "0. Duration\n",
    "1. Packets per second\n",
    "2. Bytes per second\n",
    "3. Mean packet size\n",
    "4. Standard deviation of packet sizes\n",
    "5. min, 25th, median, 75th, max packet sizes\n",
    "\n",
    "The exercise below requires a [slight modification](https://github.com/chicago-cdac/netml/pull/16/commits/79c0f930f2882c7bf042a9eb45e2a6ac413a695d) to the `netml` library (for versions <= 0.2.1) to add two additional features:\n",
    "* number of packets per flow\n",
    "* number of bytes per flow\n",
    "\n",
    "You can modify the library (line 455 of `parser.py`) to add total number of packets and bytes per flow, or optionally explore the relationships between some of the other features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d0559",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Explore the relationship between some of these features. A natural one to explore is the relationship between the number of bytes in a flow and the number of packets in a flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1559c1c",
   "metadata": {},
   "source": [
    "Train a `Linear Regression` model from scikit learn to model this relationship, and output the resulting predictions into a vector `y_hat`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ab52e",
   "metadata": {},
   "source": [
    "## Visualize Your Model\n",
    "\n",
    "Plot the relationship that your model has learned, by plotting the learned model (which should be a line), along with the points.  Label your axes!\n",
    "\n",
    "What do you notice about the relationship, and how it relates to the original points?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-illustration",
   "metadata": {},
   "source": [
    "### Evaluation: Error Computation\n",
    "\n",
    "You can compute how well your manual fit is by computing the error, in terms of residual sum of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-judge",
   "metadata": {},
   "source": [
    "## Polynomial Basis Expansion\n",
    "\n",
    "Recall that one of the benefits of a polynomial feature expansion is that it is possible to fit a linear model to a resulting polynomial expansion of the features.\n",
    "\n",
    "We will do that below.  Let's first create the regular features and then the polynomial expansion. You will need the `PolynomialFeatures` library from `sklearn.preprocessing`, as well as the `fit_transform` function to generate your feature expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098799f8",
   "metadata": {},
   "source": [
    "Train the linear regression model on the expanded set of features, and generate a new set of predictions, `y_hat_poly`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd00f969",
   "metadata": {},
   "source": [
    "Visualize your results again. What do you notice about the predicted values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9b1827",
   "metadata": {},
   "source": [
    "Evaluate your error once again. What happened to overall mean squared error?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-purple",
   "metadata": {},
   "source": [
    "## Reducing Error on Test Set\n",
    "\n",
    "In the above example, we used a polynomial basis expansion to reduce the error on the training set. But, we have no test set, so it is difficult to tell whether the model above is overfit to the training data.\n",
    "\n",
    "In order for us to tell whether this model is a good fit, we need to test it on data that the model has not yet seen. This requires splitting the data into a training set and a test set.\n",
    "\n",
    "A typical split between training data and test data might be 80% training data, 20% test data. You can also perform this process repeatedly and average the results. This process is called **cross-validation**.\n",
    "\n",
    "### Part 1\n",
    "\n",
    "1. Take a packet trace using wireshark and load it into the notebook.\n",
    "2. Perform a simple linear regression fit and a fit with basis expansion (same as above), comparing errors.\n",
    "\n",
    "The first two steps are the same as above, but you might try doing this for a larger sample so that your network traffic has more flows (i.e., data points).\n",
    "\n",
    "### Part 2\n",
    "\n",
    "3. Use functions from sklearn to split the data into training and testing. (train_test_split, or sklearn's CV function).\n",
    "4. How does model accuracy compare for different polynomial basis expansions? (n=1, 2, 3, ?) At what point is the model overfit?\n",
    "5. Can you experiment with a regularization parameter to control or reduce overfitting?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
