{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e877ee",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "With a better understanding of data representation, let's now turn to preparing data for input into a machine learning pipeline. In the case of unsupervised learning, a simple matrix-level representation can suffice for input to machine learning models; we also need accompanying labels.\n",
    "\n",
    "Often, traffic capture datasets are accompanied by labels. These labels can tell us something about the accompanying data points (i.e., flows, packets) in the traffic, and can be used to train the model for future prediction.\n",
    "\n",
    "Automated tools exist for assigning labels to traffic flows, including [pcapML](https://nprint.github.io/pcapml/). Before we use those tools, we will do some automatic preparation and labeling from an existing dataset, a log4j trace from [malware traffic analysis](https://www.malware-traffic-analysis.net/2021/12/20/index.html) and a regular trace.\n",
    "\n",
    "You can use the NetML traffic library to generate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caa27098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"scapy.runtime\").setLevel(logging.ERROR)\n",
    "\n",
    "from netml.pparser.parser import PCAP\n",
    "from netml.utils.tool import dump_data, load_data\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551ee159",
   "metadata": {},
   "source": [
    "## Load the Packet Capture Files\n",
    "\n",
    "Load the Log4j and HTTP packet capture files and extract features from the flows. You can feel free to compute features manually, although it will likely be more convenient at this point to use the `netML` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838feb39",
   "metadata": {},
   "source": [
    "## Convert the Packet Capture Into Flows\n",
    "\n",
    "Find the function in `netml` that converts the pcap file into flows. Examing the resulting data structure. What does it contain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46c6a2",
   "metadata": {},
   "source": [
    "## Explore the Flows\n",
    "\n",
    "How many flows are in each of these pcaps? (Use the `netml` library output to determine the size of each data structure.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338041ca",
   "metadata": {},
   "source": [
    "## Normalize the Shapes of Each Feature Set\n",
    "\n",
    "If you loaded the two pcaps with `netml` separately, the features will not be of the same dimension.  \n",
    "\n",
    "1. Adjust your data frames so that the two have the same number of columns.\n",
    "2. Merge (i.e., concatenate) the two data frames, but preserve the labels as a separate vector called \"target\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd91e18b",
   "metadata": {},
   "source": [
    "## Try Your Data on a Model\n",
    "\n",
    "You should now have data that can be input into a model with scikit-learn. Import the scikit-learn package (`sklearn`) and a classification model of your choice to test that you can train your model with the above data. \n",
    "\n",
    "Hint: The function you want to call is `fit`.\n",
    "\n",
    "(Note that we have not done anything here except train the model with all of the data. To evaluate the model, we will need to split the data into train and test sets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12f7b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca0c0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.loc[:,:5]\n",
    "targets = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e5ef5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0).fit(features, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57802a29",
   "metadata": {},
   "source": [
    "## Test Your Trained Model\n",
    "\n",
    "We used the entire dataset to train the model in this example (no split), and so of course the model will be well-fit to all of the data. To simply test that your trained model works, call `predict` using a feature vector that you generate by hand (e.g., from scratch, using a random set of numbers, from another pcap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "902eeb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "test = np.random.rand(6).reshape(1,-1)\n",
    "\n",
    "lr.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968bb98",
   "metadata": {},
   "source": [
    "## Looking Ahead\n",
    "\n",
    "The above exercise gives you an example of how to generate features from a packet capture, attach labels to the dataset, and train a model using the labeled data. \n",
    "\n",
    "Many other steps exist in the machine learning pipeline, including splitting the data into training and test sets, tuning model parameters, and evaluating the model. These will be the next steps we walk through."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
