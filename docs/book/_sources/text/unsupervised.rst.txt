Chapter 6: Unsupervised Learning
================================

..
    * Goals: 
        * Anomaly Detection
        * Dimensionality Reduction
        * Visualization
    * Principal Components Analysis (application: outlier detection on network traffic)
    * NetML outlier detection python framework (https://pypi.org/project/netml/)

In this chapter, we introduce *unsupervised learning*, the process by which a machine learning model can learn from *unlabeled* examples. 
The goal of unsupervised learning is to identify patterns in data that are useful for understanding the data or processing the data further.

Most data in the world is unlabeled, including most network data. For example... The prevalence of unlabeled data makes unsupervised learning a powerful tool for data analytics. 

Throughout this chapter, we will describe a variety of unsupervised learning models, using networking examples as a guide. This book does not necessarily assume youâ€™ve seen these models before, and so readers who are aiming to get basic intuition behind different models will find this chapter helpful. Readers who are already familiar with these models may still find these examples helpful, as the examples in the chapter present cases where particular models or types of models are suited to different problems, as well as cases in the networking domain where these models have been successfully applied in the past.

We organize our discussion of unsupervised learning into the following categories: (1) dimensionality reduction (i.e., models that reduce the number of features in a data set to those most useful for a task); (2) clustering (i.e., models that group data based on similarity); and (3) semi-supervised learning (i.e., models that use unsupervised techniques to prepare data for supervised learning).

Dimensionality Reduction
------------------------
Real world data sets are often high dimensional. 
They have many features and can't be easily plotted on a 2D or 3D graph. Producing useful visualizations of high dimensional data requires reducing the number of features while preserving important relationships within the data.   

The training time of most supervised ML models also increases with the number of features. 
For very high dimensional data, it may be desirable to reduce the number of features as a preprocessing step to make training computationally feasible. Dimensionality reduction algorithms perform this preprocessing step by removing or combining  features while preserving important relationships within the data.  

Both visualization and preprocessing for computational performance motivate dimensionality reduction in practice.

.. sidebar:: Activity: Dimensionality Reduction

    The :ref:`Appendix <appendix-dimensionality-reduction>` provides an
    activity to perform dimensionality reduction on a previous classification
    problem to reduce input complexity.


Principal Component Analysis
++++++++++++++++++++++++++++

The goal of principal component analysis (PCA) is to transform the data to have a new, smaller set of features derived from the original features. The new features minimize the distance that individual data points move as a result of the transformation, and maximize the variance of the data points in the target dimensionality. 
The features in the reduced dimensionality are called the *principle components*. 

..
 Restated in the language of linear algebra, PCA finds a new basis in the target dimensionality such that projecting the data into this new basis 1) minimizes the distance from the projected points to the original points and 2) maximizes the variance of the projected points. The axes in the new basis space are called the *principle components*.

Vanilla PCA is limited to linear transformations, but alternatives, such as *kernel PCA* can also account for non-linear relationships in the data. 



T-Distributed Stochastic Neighbor Embedding
+++++++++++++++++++++++++++++++++++++++++++
T-distributed stochastic neighbor embedding (T-SNE) is a dimensionality reduction algorithm that typically produces much cleaner visualizations in two or three dimensions than PCA. T-SNE is particularly useful when you want to visualize your data to gain intuition about underlying patterns that might prove informative for supervised models or clustering. 

T-SNE uses probability distributions to spread out dissimilar points in the target diminsionality while keeping similar points near each other. 
The algorithm involves three main steps: 

    1. Fit a normal distribution to the distances between pairs of points in the original data
    2. Find a mapping from the normal distribution (in the original high-dimensional space) to a T-distribution in the target dimensional space that minimizes the divergence between the distributions
    3. Select new locations for the points in the target dimensional space by drawing from this T-distribution

Because T-distributions have more probility density in the tails than a normal distribution, this spreads dissimilar points in the target dimensional space while keeping similar points in proximity. Visualizations produced using T-SNE show distinct clustering if such structure exists in the original high dimensional data. 

Autoencoders
++++++++++++
Autoencoders are unsupervised neural network models that perform dimensionality reduction. 

An autoencoder network has input and output layers that are the same size as the number of features in the data. 
The intermediate layers of the network have an "hourglass" shape, with decreasing numbers of nodes from the input layer to a central "encoding" layer and increasing numbers of nodes from the encoding layer to the output layer. 
This reduction in layer size forces information loss as each example passes through the autoencoder, since the encoding layer cannot retain all features of the input data. 

Autoencoders are trained to reproduce their input as closely as possible in their output. In other words, the sama data is used as both the training examples and the training labels. This causes the network to find parameters such that the encoding layer retains the most important information about the input features and serves as the target low-dimensional representation. The size of the encoding layer is selected beforehand to match the target dimensionality of the dimensionality reduction process. 

Clustering
----------
Clustering algorithms group data points by similarity, identifying latent structure in the dataset.  

.. sidebar:: Activity: Clustering

    The :ref:`Appendix <appendix-clustering>` provides an activity to apply
    different clustering algorithms on a network traffic trace that contains
    both benign and attack traffic.


K-Means
+++++++
K-means is a fairly simple algorithm that clusters a dataset into K groups:

    1. Choose a target number of clusters K 
    2. Choose K random points as starting centroids (points that define the center of a cluster)
    3. Assign all other points in the data set to the cluster with the closest centroid
    4. Update the centroids to the mean locations of each the points in their cluster
    5. Repeat steps 3 and 4 until the centroid locations stop changing. 

This algorithm is fast and always converges but has some drawbacks. 
Most importantly, you have to choose the number of clusters. This can be straightfoward if you have existing knowledge about the structure of the dataset. For example, if you have a network traffic dataset that you want to cluster into TCP and UDP traffic, you might choose K=2 and then check whether the clusters match protocols 

If you don't know the number of clusters, you can run K-means with increasing cluster numbers to see which produces the cleanest clustering, but you might be better off choosing a different algorithm. 
K-means also performs poorly for non-spherical clusters or clusters of varying density. If your data falls into either of these categories, you might also be better off choosing a different algorithm. 

Gaussian Mixture Models
+++++++++++++++++++++++

This alternative to K-means defines clusters not just by their center point (centroid) but also by their variance.
This assumes that the underlying clusters in the data follow normal distributions, with each cluster having a mean and variance. While this may not be strictly true for some data, it is often a good approximation due to the central limit theorem. 

The process of applying Gaussain mixture models (GMM) is fairly similar to K-means. You must choose a number of centroids (or repeat the model iteratively with different number of centroids), and the model will find the centroid means and variances that best fit your data. 

A Gaussian Mixture Model is also a generative model, because you can draw new data points from the underlying distributions. This allows you to create new data with similar characteristics as your training data, useful for many applications (e.g. training set augmentation). 


Density-Based Spatial Clustering of Applications with Noise (DBSCAN)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
DBSCAN uses datapoint density to identify clusters similarly to how humans visually identify clusters of points on a plot. 
High-density groups of points (groups with relatively many points a relatively small distance from each other) become clusters. These clusters are defined by a core example and a neighborhood distance. 

DBSCAN has a lot of advantages. It does not force you to choose the number of clusters beforehand; it will find as many groups of nearby dense points as it can. It also works for datasets that aren't spherical. 
DBSCAN is frequently used for anomaly detection, because it can automatically identify points that don't fit in to any existing clusters. 
This is very useful in networks problems, such as malicious traffic detection, where identifying unusual examples is valuable. 

DBSCAN has some disadvantages due to its dependency on data density. If you have some clusters that are tightly packed and other clusters that are more spread out, DBSCAN may be unable to achieve the desired clustering. DBSCAN can also struggle with high dimensional data because the ''curse of dimensionality'' means that all data points appear far apart in high dimensional space. 

Hierarchical Clustering
+++++++++++++++++++++++
Hierarchical clustering approaches contruct a ''dendrogram,'' or tree diagram, that illustrates how examples can be progressively grouped by an arbitrary similarity metric (e.g. Euclidean distance). This provides a really nice visual representation of your dataset including which points are more closely related than others.

If you want to create a specific clustering from a hierarchical dendrogram, you can divide the tree at a specific level of similarity, and all examples grouped at that position become clusters. 


Semi-Supervised Learning
------------------------
Semi-supervised learning leverages unsupervised learning to speed up the process of providing ground-truth labels for eventual supervised ML. In nearly all fields of ML, manual labeling is tedious. This is especially true for networks. The idea behind semi-supervised learning is that you combine a small number of manual labels with a clustering algorithm to produce a fully labeled training dataset. 

You start by using a clustering algorithm to group the unlabeled training data. You then manually label a few randomly selected points from each cluster and propagate the most frequent manual label in each cluster to the other points in the cluster. This gives you a fully labeled data set even though you only had to manually label a few points.

Ideally, the clustering algorithm produces clusters in which all points are from the same class, but in practice, some clusters may have examples from multiple classes. You can perform semi-supervised learning recursively to address this issue. For example, if one cluster has points with several disparate manual labels, you can re-run the clustering algorithm on just this cluster, identifying sub-clusters that may correspond to a single class.
