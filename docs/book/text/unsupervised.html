

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Chapter 6: Unsupervised Learning &mdash; Applied Machine Learning for Networking Version 0.1-dev documentation</title>
  

  
  
    <link rel="shortcut icon" href="../static/bridge.ico"/>
  
  
  

  
  <script type="text/javascript" src="../static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../static/documentation_options.js"></script>
        <script src="../static/jquery.js"></script>
        <script src="../static/underscore.js"></script>
        <script src="../static/doctools.js"></script>
        <script src="../static/language_data.js"></script>
        <script src="https://www.googletagmanager.com/gtag/js?id=G-QLSP3FJWGT"></script>
        <script >
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-QLSP3FJWGT');
</script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../static/css/rtd_theme_mods.css" type="text/css" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 7: Machine Learning in Network Deployment" href="automation.html" />
    <link rel="prev" title="Chapter 5: Supervised Learning Models" href="supervised.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Applied Machine Learning for Networking
          

          
          </a>

          
            
            
              <div class="version">
                Version 0.1-dev
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Table of Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Chapter 1:  Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="motivation.html">Chapter 2: Motivating Problems and Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="measurement.html">Chapter 3: Network Measurement</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipelines.html">Chapter 4: Machine Learning Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="supervised.html">Chapter 5: Supervised Learning Models</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Chapter 6: Unsupervised Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#dimensionality-reduction">Dimensionality Reduction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#principle-component-analysis">Principle Component Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="#t-distributed-stochastic-neighbor-embedding">T-Distributed Stochastic Neighbor Embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="#autoencoders">Autoencoders</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#clustering">Clustering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#k-means">K-Means</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gaussian-mixture-models">Gaussian Mixture Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#density-based-spatial-clustering-of-applications-with-noise-dbscan">Density-Based Spatial Clustering of Applications with Noise (DBSCAN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hierarchical-clustering">Hierarchical Clustering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#semi-supervised-learning">Semi-Supervised Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="automation.html">Chapter 7:  Machine Learning in Network Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="concerns.html">Chapter 8:  Privacy, Legal, &amp; Ethical Concerns</a></li>
<li class="toctree-l1"><a class="reference internal" href="future.html">Chapter 9:  Looking Ahead</a></li>
<li class="toctree-l1"><a class="reference internal" href="../README.html">About The Book</a></li>
<li class="toctree-l1"><a class="reference internal" href="../authors.html">About The Authors</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Applied Machine Learning for Networking</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Chapter 6: Unsupervised Learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/text/unsupervised.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <div class="rst-breadcrumbs-buttons" role="navigation" aria-label="breadcrumb navigation">
      
        <a href="automation.html" class="btn btn-neutral float-right" title="Chapter 7: Machine Learning in Network Deployment" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="supervised.html" class="btn btn-neutral float-left" title="Chapter 5: Supervised Learning Models" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
  </div>
  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="chapter-6-unsupervised-learning">
<h1>Chapter 6: Unsupervised Learning<a class="headerlink" href="#chapter-6-unsupervised-learning" title="Permalink to this headline">¶</a></h1>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt>Goals:</dt><dd><ul>
<li><p>Anomaly Detection</p></li>
<li><p>Dimensionality Reduction</p></li>
<li><p>Visualization</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Principal Components Analysis (application: outlier detection on network traffic)</p></li>
<li><p>NetML outlier detection python framework (<a class="reference external" href="https://pypi.org/project/netml/">https://pypi.org/project/netml/</a>)</p></li>
</ul>
</div></blockquote>
<p>Most of the data in the world is unlabeled.
This is often the case for networks data as well.
EXAMPLES</p>
<p>The goal of unsupervisd learning is to identify patterns in unlabeled data that are useful for understanding the data or processing the data further.
The two main types of unsupervised learning relevant for networks problems are <em>dimensionality reduction</em>, <em>clustering</em>, and <em>semi-supervised learning</em>.</p>
<div class="section" id="dimensionality-reduction">
<h2>Dimensionality Reduction<a class="headerlink" href="#dimensionality-reduction" title="Permalink to this headline">¶</a></h2>
<p>Real world data sets are often high dimensional.
They have many features and can’t be easily plotted on a 2D or 3D graph. Producing useful visualizations of high dimensional data requires reducing the number of features while preserving important relationships within the data.</p>
<p>The training time of most supervised ML models also increases with the number of features.
For especially high dimensional data, it may be desirable to reduce the number of features as a preprocessing step to make training computationally feasible. Unsupervised dimensionality reduction algorithms will perform this preprocessing step by removing or combining less predictive features.</p>
<p>Both visualization and preprocessing for computational performance motivate dimensionality reduction in practice.</p>
<div class="section" id="principle-component-analysis">
<h3>Principle Component Analysis<a class="headerlink" href="#principle-component-analysis" title="Permalink to this headline">¶</a></h3>
<p>The goal of principal component analysis is to find a new basis in the target dimensionality such that projecting the data into this new basis minimizes the distance from the projected points to the original points and maximizes the variance of the projected points.
The axes in the new basis space are called the <em>principle components</em>.
Vanilla PCA is limited to linear transformations, but alternatives, such as <em>kernel PCA</em> can also account for non-linear relationships in the data.</p>
</div>
<div class="section" id="t-distributed-stochastic-neighbor-embedding">
<h3>T-Distributed Stochastic Neighbor Embedding<a class="headerlink" href="#t-distributed-stochastic-neighbor-embedding" title="Permalink to this headline">¶</a></h3>
<p>T-distributed stochastic neighbor embedding is a dimensionality reduction algorithm that typically produces much cleaner visualizations than PCA.
T-SNE works by (1) fitting a Gaussian distribution to the distances between pairs of points in the original high-dimensional space, (2) Use gradient descent to find a mapping between that Gaussian to a T-distribution in the target dimensional space that minimizes the divergence between the distributions, (3) choosing locations of points by drawing from this T-distribution. This has the effect of keeping similar points in the original space closer together in the target space while spreading dissimilar points further apart in the target space (because the T-distribution has more probility density in the tails than the Gaussian)</p>
</div>
<div class="section" id="autoencoders">
<h3>Autoencoders<a class="headerlink" href="#autoencoders" title="Permalink to this headline">¶</a></h3>
<p>Autoencoders are unsupervised neural network models that perform dimensionality reductions.
The idea is that you have a network with an input layer and output layer that are the same size as your high-dimensional data. The intermediate hidden layers of the network have an “hourglass” shape, with a layer with an output size of the target dimension in the middle.
The network is trained to recreate the output from the input, however, the reduction in size in the hidden layers forces the network to lose information (it can’t just pass a full representation of all features through every layer).
The training forces te network to find parameters such that the output of the encoding layer provides the most information about all input features as possible so the data can be reproduced with the highest fidelity.</p>
</div>
</div>
<div class="section" id="clustering">
<h2>Clustering<a class="headerlink" href="#clustering" title="Permalink to this headline">¶</a></h2>
<p>Clustering algorithms group data points by similarity, identifying latent structure in the dataset.</p>
<div class="section" id="k-means">
<h3>K-Means<a class="headerlink" href="#k-means" title="Permalink to this headline">¶</a></h3>
<p>K-means is a fairly simple algorithm, it can be defined in four steps.
(1) choose a target number of clusters K. (2) Choose K random points as starting centroids. (3) Assign all of the other points in the data set to the closest one of those centroids. (4) Update the centroids to the mean locations of each of the K clusters. (5) Repeat steps 3 and 4 until the centroid locations stop changing.</p>
<p>This algorithm is fast and always converges but has some drawbacks.
You have to choose the number of clusters. In some problems this is easy. If you’re collecting data about handwritten digits, you know that there’s going to be nine digits, so you just pick nine clusters.
If you don’t know the number of clusters, you can run K-means with increasing cluster numbers to see which produces the cleanest clustering, but you might be better off choosing a different algorithm.
K-means also performs poorly for non-spherical clusters or clusters of varying density - other reasons to choose another clustering algorithm.</p>
</div>
<div class="section" id="gaussian-mixture-models">
<h3>Gaussian Mixture Models<a class="headerlink" href="#gaussian-mixture-models" title="Permalink to this headline">¶</a></h3>
<p>This alternative to K-means defined clusters not just by their center point (centroid) but also by their variance.
This assumes that the underlying clusters in the data follow normal distributions, with each cluster having a mean and variance. While this may not be strictly true for some data, it is often a good approximation due to the central limit theorem.</p>
<p>The process of applying GMM is fairly similar to K-means. You must choose a number of centroids (or repeat the model iteratively with different number of centroids), and the model will find the centroid means and variances that best fit your data.</p>
<p>A Gaussian Mixture Model is also a generative model, because you can draw new data points from the underlying distributions. This allows you to create new data with similar characteristics as your training data, useful for many applications (e.g. training set augmentation)</p>
</div>
<div class="section" id="density-based-spatial-clustering-of-applications-with-noise-dbscan">
<h3>Density-Based Spatial Clustering of Applications with Noise (DBSCAN)<a class="headerlink" href="#density-based-spatial-clustering-of-applications-with-noise-dbscan" title="Permalink to this headline">¶</a></h3>
<p>DBSCAN uses datapoint density to identify clusters similarly to how humans identify groups of points visually on a plot.
High-density groups of points (groups with relatively many points a relatively small distance from each other) become clusters. These clusters are defined by a core example and a neighborhood distance.</p>
<p>DBSCAN has a lot of advantages. This does not force you to choose the number of clusters beforehand, it will just find as many groups of nearby dense points as it can. It works for datasets that aren’t spherical.
DBSCAN is frequently used for anomaly detection, because it can automatically identify points that don’t fit in to an existing cluster.
This is very useful in networks problems where identifying unusual examples is valuable.</p>
<p>DBSCAN has some disadvantages due to its dependency on data density. If you have some clusters that are tightly packed and other clusters that are a lot more spread out, it may be difficult or impossible to tune the hyperparameters to achieve the desired clustering. DBSCAN can also struggle with high dimensional data because the ‘’curse of dimensionality’’ means that all data points appear far apart in high dimensional space.</p>
</div>
<div class="section" id="hierarchical-clustering">
<h3>Hierarchical Clustering<a class="headerlink" href="#hierarchical-clustering" title="Permalink to this headline">¶</a></h3>
<p>Hierarchical clustering approaches contruct a ‘’dendrogram’’, or tree diagram, that illustrates how examples can be progressively grouped by an arbitrary similarity metric. This provides a really nice visual representation of your dataset including which points are more closely related than others.</p>
<p>If you want to create a specific clustering from a hierarchical dendrogram, you can draw a horizontal line to divide the tree, and all examples grouped at that position in the tree become clusters.</p>
</div>
</div>
<div class="section" id="semi-supervised-learning">
<h2>Semi-Supervised Learning<a class="headerlink" href="#semi-supervised-learning" title="Permalink to this headline">¶</a></h2>
<p>Semi-supervised learning leverages unsupervised learning to speed up the process of providing ground-truth labels for eventual supervised ML. In nearly all fields of ML, manual labeling is tedious. This is especially true for networks.
The idea behind semi-supervised learning is that you combine a small number of manual labels with a clustering algorithm. Hopefully, the clustering algorithm will group the data into clusters with a few manually labeled examples per cluster. You can then propagate the manual labels to the other points in the cluster. This gives you a fully labeled data set even though you only had to manually label a couple of points.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="automation.html" class="btn btn-neutral float-right" title="Chapter 7: Machine Learning in Network Deployment" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="supervised.html" class="btn btn-neutral float-left" title="Chapter 5: Supervised Learning Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>