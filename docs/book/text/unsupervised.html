<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Chapter 6: Unsupervised Learning &mdash; Machine Learning for Networking Version 0.1 documentation</title><link rel="stylesheet" href="../static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../static/css/rtd_theme_mods.css" type="text/css" />
    <link rel="shortcut icon" href="../static/bridge.ico"/>
  <!--[if lt IE 9]>
    <script src="../static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../static/documentation_options.js"></script>
        <script src="../static/jquery.js"></script>
        <script src="../static/underscore.js"></script>
        <script src="../static/doctools.js"></script>
        <script src="../static/language_data.js"></script>
        <script src="https://www.googletagmanager.com/gtag/js?id=G-QLSP3FJWGT"></script>
        <script >
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-QLSP3FJWGT');
</script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "tex2jax_ignore|mathjax_ignore|document", "processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script src="../static/js/theme.js"></script>
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 7: Deployment Considarations" href="automation.html" />
    <link rel="prev" title="Chapter 5: Supervised Learning" href="supervised.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Machine Learning for Networking
          </a>
              <div class="version">
                Version 0.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Table of Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Chapter 1:  Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="motivation.html">Chapter 2: Motivating Problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="measurement.html">Chapter 3: Data Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Chapter 4: Machine Learning Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="supervised.html">Chapter 5: Supervised Learning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Chapter 6: Unsupervised Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#dimensionality-reduction">Dimensionality Reduction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#principal-component-analysis">Principal Component Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="#t-distributed-stochastic-neighbor-embedding">T-Distributed Stochastic Neighbor Embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="#autoencoders">Autoencoders</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#clustering">Clustering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#k-means">K-Means</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gaussian-mixture-models">Gaussian Mixture Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#density-based-spatial-clustering-of-applications-with-noise-dbscan">Density-Based Spatial Clustering of Applications with Noise (DBSCAN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hierarchical-clustering">Hierarchical Clustering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#semi-supervised-learning">Semi-Supervised Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="automation.html">Chapter 7:  Deployment Considarations</a></li>
<li class="toctree-l1"><a class="reference internal" href="future.html">Chapter 8:  Looking Ahead</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">Appendix: Activities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../README.html">About The Book</a></li>
<li class="toctree-l1"><a class="reference internal" href="../authors.html">About The Authors</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Machine Learning for Networking</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Chapter 6: Unsupervised Learning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/text/unsupervised.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul><div class="rst-breadcrumbs-buttons" role="navigation" aria-label="Sequential page navigation">
        <a href="supervised.html" class="btn btn-neutral float-left" title="Chapter 5: Supervised Learning" accesskey="p"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="automation.html" class="btn btn-neutral float-right" title="Chapter 7: Deployment Considarations" accesskey="n">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
  </div>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="chapter-6-unsupervised-learning">
<h1>Chapter 6: Unsupervised Learning<a class="headerlink" href="#chapter-6-unsupervised-learning" title="Permalink to this headline">¶</a></h1>
<p>In this chapter, we introduce <em>unsupervised learning</em>, the process by which a machine learning model can learn from <em>unlabeled</em> examples.
The goal of unsupervised learning is to identify patterns in data that are useful for understanding the data or processing the data further.</p>
<p>Most data in the world is unlabeled, including most network data. For example… The prevalence of unlabeled data makes unsupervised learning a powerful tool for data analytics.</p>
<p>Throughout this chapter, we will describe a variety of unsupervised learning models, using networking examples as a guide. This book does not necessarily assume you’ve seen these models before, and so readers who are aiming to get basic intuition behind different models will find this chapter helpful. Readers who are already familiar with these models may still find these examples helpful, as the examples in the chapter present cases where particular models or types of models are suited to different problems, as well as cases in the networking domain where these models have been successfully applied in the past.</p>
<p>We organize our discussion of unsupervised learning into the following categories: (1) dimensionality reduction (i.e., models that reduce the number of features in a data set to those most useful for a task); (2) clustering (i.e., models that group data based on similarity); and (3) semi-supervised learning (i.e., models that use unsupervised techniques to prepare data for supervised learning).</p>
<div class="section" id="dimensionality-reduction">
<h2>Dimensionality Reduction<a class="headerlink" href="#dimensionality-reduction" title="Permalink to this headline">¶</a></h2>
<p>Real world data sets are often high dimensional.
They have many features and can’t be easily plotted on a 2D or 3D graph. Producing useful visualizations of high dimensional data requires reducing the number of features while preserving important relationships within the data.</p>
<p>The training time of most supervised ML models also increases with the number of features.
For very high dimensional data, it may be desirable to reduce the number of features as a preprocessing step to make training computationally feasible. Dimensionality reduction algorithms perform this preprocessing step by removing or combining  features while preserving important relationships within the data.</p>
<p>Both visualization and preprocessing for computational performance motivate dimensionality reduction in practice.</p>
<div class="sidebar">
<p class="sidebar-title">Activity: Dimensionality Reduction</p>
<p>The <a class="reference internal" href="appendix.html#appendix-dimensionality-reduction"><span class="std std-ref">Appendix</span></a> provides an
activity to perform dimensionality reduction on a previous classification
problem to reduce input complexity.</p>
</div>
<div class="section" id="principal-component-analysis">
<h3>Principal Component Analysis<a class="headerlink" href="#principal-component-analysis" title="Permalink to this headline">¶</a></h3>
<p>The goal of principal component analysis (PCA) is to transform the data to have a new, smaller set of features derived from the original features. The new features minimize the distance that individual data points move as a result of the transformation, and maximize the variance of the data points in the target dimensionality.
The features in the reduced dimensionality are called the <em>principle components</em>.</p>
<p>Vanilla PCA is limited to linear transformations, but alternatives, such as <em>kernel PCA</em> can also account for non-linear relationships in the data.</p>
</div>
<div class="section" id="t-distributed-stochastic-neighbor-embedding">
<h3>T-Distributed Stochastic Neighbor Embedding<a class="headerlink" href="#t-distributed-stochastic-neighbor-embedding" title="Permalink to this headline">¶</a></h3>
<p>T-distributed stochastic neighbor embedding (T-SNE) is a dimensionality reduction algorithm that typically produces much cleaner visualizations in two or three dimensions than PCA. T-SNE is particularly useful when you want to visualize your data to gain intuition about underlying patterns that might prove informative for supervised models or clustering.</p>
<p>T-SNE uses probability distributions to spread out dissimilar points in the target diminsionality while keeping similar points near each other.
The algorithm involves three main steps:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Fit a normal distribution to the distances between pairs of points in the original data</p></li>
<li><p>Find a mapping from the normal distribution (in the original high-dimensional space) to a T-distribution in the target dimensional space that minimizes the divergence between the distributions</p></li>
<li><p>Select new locations for the points in the target dimensional space by drawing from this T-distribution</p></li>
</ol>
</div></blockquote>
<p>Because T-distributions have more probility density in the tails than a normal distribution, this spreads dissimilar points in the target dimensional space while keeping similar points in proximity. Visualizations produced using T-SNE show distinct clustering if such structure exists in the original high dimensional data.</p>
</div>
<div class="section" id="autoencoders">
<h3>Autoencoders<a class="headerlink" href="#autoencoders" title="Permalink to this headline">¶</a></h3>
<p>Autoencoders are unsupervised neural network models that perform dimensionality reduction.</p>
<p>An autoencoder network has input and output layers that are the same size as the number of features in the data.
The intermediate layers of the network have an “hourglass” shape, with decreasing numbers of nodes from the input layer to a central “encoding” layer and increasing numbers of nodes from the encoding layer to the output layer.
This reduction in layer size forces information loss as each example passes through the autoencoder, since the encoding layer cannot retain all features of the input data.</p>
<p>Autoencoders are trained to reproduce their input as closely as possible in their output. In other words, the sama data is used as both the training examples and the training labels. This causes the network to find parameters such that the encoding layer retains the most important information about the input features and serves as the target low-dimensional representation. The size of the encoding layer is selected beforehand to match the target dimensionality of the dimensionality reduction process.</p>
</div>
</div>
<div class="section" id="clustering">
<h2>Clustering<a class="headerlink" href="#clustering" title="Permalink to this headline">¶</a></h2>
<p>Clustering algorithms group data points by similarity, identifying latent structure in the dataset.</p>
<div class="sidebar">
<p class="sidebar-title">Activity: Clustering</p>
<p>The <a class="reference internal" href="appendix.html#appendix-clustering"><span class="std std-ref">Appendix</span></a> provides an activity to apply
different clustering algorithms on a network traffic trace that contains
both benign and attack traffic.</p>
</div>
<div class="section" id="k-means">
<h3>K-Means<a class="headerlink" href="#k-means" title="Permalink to this headline">¶</a></h3>
<p>K-means is a fairly simple algorithm that clusters a dataset into K groups:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Choose a target number of clusters K</p></li>
<li><p>Choose K random points as starting centroids (points that define the center of a cluster)</p></li>
<li><p>Assign all other points in the data set to the cluster with the closest centroid</p></li>
<li><p>Update the centroids to the mean locations of each the points in their cluster</p></li>
<li><p>Repeat steps 3 and 4 until the centroid locations stop changing.</p></li>
</ol>
</div></blockquote>
<p>This algorithm is fast and always converges but has some drawbacks.
Most importantly, you have to choose the number of clusters. This can be straightfoward if you have existing knowledge about the structure of the dataset. For example, if you have a network traffic dataset that you want to cluster into TCP and UDP traffic, you might choose K=2 and then check whether the clusters match protocols</p>
<p>If you don’t know the number of clusters, you can run K-means with increasing cluster numbers to see which produces the cleanest clustering, but you might be better off choosing a different algorithm.
K-means also performs poorly for non-spherical clusters or clusters of varying density. If your data falls into either of these categories, you might also be better off choosing a different algorithm.</p>
</div>
<div class="section" id="gaussian-mixture-models">
<h3>Gaussian Mixture Models<a class="headerlink" href="#gaussian-mixture-models" title="Permalink to this headline">¶</a></h3>
<p>This alternative to K-means defines clusters not just by their center point (centroid) but also by their variance.
This assumes that the underlying clusters in the data follow normal distributions, with each cluster having a mean and variance. While this may not be strictly true for some data, it is often a good approximation due to the central limit theorem.</p>
<p>The process of applying Gaussain mixture models (GMM) is fairly similar to K-means. You must choose a number of centroids (or repeat the model iteratively with different number of centroids), and the model will find the centroid means and variances that best fit your data.</p>
<p>A Gaussian Mixture Model is also a generative model, because you can draw new data points from the underlying distributions. This allows you to create new data with similar characteristics as your training data, useful for many applications (e.g. training set augmentation).</p>
</div>
<div class="section" id="density-based-spatial-clustering-of-applications-with-noise-dbscan">
<h3>Density-Based Spatial Clustering of Applications with Noise (DBSCAN)<a class="headerlink" href="#density-based-spatial-clustering-of-applications-with-noise-dbscan" title="Permalink to this headline">¶</a></h3>
<p>DBSCAN uses datapoint density to identify clusters similarly to how humans visually identify clusters of points on a plot.
High-density groups of points (groups with relatively many points a relatively small distance from each other) become clusters. These clusters are defined by a core example and a neighborhood distance.</p>
<p>DBSCAN has a lot of advantages. It does not force you to choose the number of clusters beforehand; it will find as many groups of nearby dense points as it can. It also works for datasets that aren’t spherical.
DBSCAN is frequently used for anomaly detection, because it can automatically identify points that don’t fit in to any existing clusters.
This is very useful in networks problems, such as malicious traffic detection, where identifying unusual examples is valuable.</p>
<p>DBSCAN has some disadvantages due to its dependency on data density. If you have some clusters that are tightly packed and other clusters that are more spread out, DBSCAN may be unable to achieve the desired clustering. DBSCAN can also struggle with high dimensional data because the ‘’curse of dimensionality’’ means that all data points appear far apart in high dimensional space.</p>
</div>
<div class="section" id="hierarchical-clustering">
<h3>Hierarchical Clustering<a class="headerlink" href="#hierarchical-clustering" title="Permalink to this headline">¶</a></h3>
<p>Hierarchical clustering approaches contruct a ‘’dendrogram,’’ or tree diagram, that illustrates how examples can be progressively grouped by an arbitrary similarity metric (e.g. Euclidean distance). This provides a really nice visual representation of your dataset including which points are more closely related than others.</p>
<p>If you want to create a specific clustering from a hierarchical dendrogram, you can divide the tree at a specific level of similarity, and all examples grouped at that position become clusters.</p>
</div>
</div>
<div class="section" id="semi-supervised-learning">
<h2>Semi-Supervised Learning<a class="headerlink" href="#semi-supervised-learning" title="Permalink to this headline">¶</a></h2>
<p>Semi-supervised learning leverages unsupervised learning to speed up the process of providing ground-truth labels for eventual supervised ML. In nearly all fields of ML, manual labeling is tedious. This is especially true for networks. The idea behind semi-supervised learning is that you combine a small number of manual labels with a clustering algorithm to produce a fully labeled training dataset.</p>
<p>You start by using a clustering algorithm to group the unlabeled training data. You then manually label a few randomly selected points from each cluster and propagate the most frequent manual label in each cluster to the other points in the cluster. This gives you a fully labeled data set even though you only had to manually label a few points.</p>
<p>Ideally, the clustering algorithm produces clusters in which all points are from the same class, but in practice, some clusters may have examples from multiple classes. You can perform semi-supervised learning recursively to address this issue. For example, if one cluster has points with several disparate manual labels, you can re-run the clustering algorithm on just this cluster, identifying sub-clusters that may correspond to a single class.</p>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="supervised.html" class="btn btn-neutral float-left" title="Chapter 5: Supervised Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="automation.html" class="btn btn-neutral float-right" title="Chapter 7: Deployment Considarations" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>