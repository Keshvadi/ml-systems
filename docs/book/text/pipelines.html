

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Chapter 4: Machine Learning Pipeline &mdash; Applied Machine Learning for Networking Version 0.1-dev documentation</title>
  

  
  
    <link rel="shortcut icon" href="../static/bridge.ico"/>
  
  
  

  
  <script type="text/javascript" src="../static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../static/documentation_options.js"></script>
        <script src="../static/jquery.js"></script>
        <script src="../static/underscore.js"></script>
        <script src="../static/doctools.js"></script>
        <script src="../static/language_data.js"></script>
        <script src="https://www.googletagmanager.com/gtag/js?id=G-QLSP3FJWGT"></script>
        <script >
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-QLSP3FJWGT');
</script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../static/css/rtd_theme_mods.css" type="text/css" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 5: Supervised Learning Models" href="supervised.html" />
    <link rel="prev" title="Chapter 3: Network Measurement" href="measurement.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Applied Machine Learning for Networking
          

          
          </a>

          
            
            
              <div class="version">
                Version 0.1-dev
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Table of Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Chapter 1:  Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="motivation.html">Chapter 2: Motivating Problems and Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="measurement.html">Chapter 3: Network Measurement</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Chapter 4: Machine Learning Pipeline</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#data-acquisition">Data Acquisition</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-preparation">Data Preparation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#examples-features-and-labels">Examples: Features and Labels</a></li>
<li class="toctree-l3"><a class="reference internal" href="#types-of-labels">Types of Labels</a></li>
<li class="toctree-l3"><a class="reference internal" href="#train-test-sets">Train &amp; Test Sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="#validation-sets-and-cross-validation">Validation Sets and Cross-Validation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-training">Model Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#error-minimization-and-gradient-descent">Error Minimization and Gradient Descent</a></li>
<li class="toctree-l3"><a class="reference internal" href="#overfitting">Overfitting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-evaluation">Model Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#supervised-metrics">Supervised Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#unsupervised-metrics">Unsupervised Metrics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="supervised.html">Chapter 5: Supervised Learning Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="unsupervised.html">Chapter 6: Unsupervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="automation.html">Chapter 7:  Machine Learning in Network Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="concerns.html">Chapter 8:  Privacy, Legal, &amp; Ethical Concerns</a></li>
<li class="toctree-l1"><a class="reference internal" href="future.html">Chapter 9:  Looking Ahead</a></li>
<li class="toctree-l1"><a class="reference internal" href="../README.html">About The Book</a></li>
<li class="toctree-l1"><a class="reference internal" href="../authors.html">About The Authors</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Applied Machine Learning for Networking</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Chapter 4: Machine Learning Pipeline</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/text/pipelines.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <div class="rst-breadcrumbs-buttons" role="navigation" aria-label="breadcrumb navigation">
      
        <a href="supervised.html" class="btn btn-neutral float-right" title="Chapter 5: Supervised Learning Models" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="measurement.html" class="btn btn-neutral float-left" title="Chapter 3: Network Measurement" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
  </div>
  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="chapter-4-machine-learning-pipeline">
<h1>Chapter 4: Machine Learning Pipeline<a class="headerlink" href="#chapter-4-machine-learning-pipeline" title="Permalink to this headline">¶</a></h1>
<p>Now that we have a good understanding of how to gather different types of
network data through measurement, we can begin to think about how to transform
this data for input to machine learning models. The process of machine
learning consitutes a pipeline that involves a sequence of steps, including data
gathering and prepration, transformation and labeling, splitting of datasets
for model training and tuning, and model evaluation.</p>
<p>Towards this goal, this chapter discusses the process of training machine
learning models from network traffic—what we often refer to as the <em>machine
learning pipeline</em>. Generally, the machine learning pipeline applies to models
that learn from labeled examples (<em>supervised learning</em>) as well as from data
that does not have corresponding labels (<em>unsupervised learning</em>). We will
focus initially in this chapter on how the pipeline applies to supervised
learning, before turning to how it can be applied in the unsupervised context.</p>
<div class="section" id="data-acquisition">
<h2>Data Acquisition<a class="headerlink" href="#data-acquisition" title="Permalink to this headline">¶</a></h2>
<p>All machine learning pipelines begin with the collection of relevant data.
Data can come from many different sources and can vary wildly in complexity.
Most importantly, the data you gather, and how you represent it through
collections of features, should accurately represent the underlying phenomena
you are trying to model.  Machine learning algorithms learn primarily through
inductive reasoning, i.e., they learn general rules by observing specific
instances.  If the specific instances in your data are not adequately
representative of the general phenomenon you are trying to model, the ML
algorithm will not be able to learn the right general rules and any
predictions based on the model are less likely to be correct.  Thus, decisions
about what data to acquire and how to acquire it can often be critical to
whether a model is accurate in practice, as well as whether the model is
efficient and robust enough to be deployed in practice.</p>
<p>In this respect, <em>domain knowledge</em> is particularly important in the context
of machine learning. For example, knowing how patterns of malicious web
requests as part of a denial of service attack or vulnerability scan would
exhibit different characteristics from legitimate web requests is essential
for thinking about the types of features that may ultimately be good for the
models you ultimately train. More generally, when designing machine learning
pipelines for specific tasks, domain knowledge about the problem you are
trying to model can increase the likelihood that the model you design could
work well in practice.</p>
<p>When beginning a ML pipeline, it is common to ask, “how much data is necessary
to achieve high-quality results?” Unfortunately, answering this question is
not straightforward, as it depends the phenomenon you are trying to model, the
machine learning algorithm you select, and the quality of the data you are
able to gather. Conceptually, you will need to collect enough data such that
all of the relevant variability in the underlying phenomenon is represented in
the data.</p>
<p>From the perspective of model accuracy, identifying features of the traffic
that are likely to result in accurate predictions often requires knowledge of
the underlying phenomena; from the perspective of practical deployment,
considerations for data acquisition can also go beyond accuracy, because some
features are more costly or cumbersome than others. For example, packet traces
may contain significantly more information that statistical summaries of
network traffic, yet packet traces are orders of magnitude larger, resulting
in higher capture overhead, larger memory and storage costs, longer model
training time, and so forth. On the other hand, summary statistics are more
efficient to collect and store, and could reduce training time, yet they may
obscure certain features or characteristics which could ultimately degrade
model accuracy. Determining which feature subsets strike the appropriate
balance between model accuracy and systems-related modeling costs is a general
practical challenge, and the best ways to optimize for this tradeoff remains
an open problem.</p>
</div>
<div class="section" id="data-preparation">
<h2>Data Preparation<a class="headerlink" href="#data-preparation" title="Permalink to this headline">¶</a></h2>
<p>After acquiring data, the next step is <em>preparing</em> the data for eventual input
to models. This process of <em>data preparation</em> invovles organizing the data into
observations, or <em>examples</em>; generating <em>features</em> and a <em>label</em> for each
observation; and preparing the dataset by dividing the data into training and
test sets (typically with an additional sub-division of the training set to
tune model parameters). In the context of networking, a example might be an
individual packet, an entire traffic flow (with associated summary
statistics), information about a connection or transaction, and so forth.</p>
<div class="section" id="examples-features-and-labels">
<h3>Examples: Features and Labels<a class="headerlink" href="#examples-features-and-labels" title="Permalink to this headline">¶</a></h3>
<p>As mentioned, the goal of supervised machine learning is to use labeled data to
train a model that can the predict correct labels (sometimes referred to as
<em>targets</em> or target predictions) for data points that the model did not see
when it was being trained.</p>
<p>As a human analogue, the process of supervised machine learning is akin
learning how to solve a general problem by seeing the correct answer to
several specific instances and identifying patterns that are useful for
solving any instance of the task. For example, when driving, you know that the
correct action when seeing a stop light is to bring your car to a halt, even
though you may have never seen that specific stoplight at that instance ever
before. In the context of networking, then, many such examples exist. A
machine learning model might automatically recognize an attack, or traffic
from a specific application, even though the model has never before seen that
specific traffic trace.</p>
<p>Any use of supervised learning to solve a problem must thus necessarily
involve the collection of <em>labeled training data</em>. Training data is usually
divided into <em>training examples</em> and <em>training labels</em>. The training examples
are observations or data points. The training labels consist of the <em>correct</em>
categories or values that a trained machine learning algorithm should assign
to each of the training examples.</p>
<p>Many supervised machine learning problems represent the training examples as a
two-dimensional matrix. Each row of the matrix contains a single <em>example</em>.
Each column of the matrix corresponds to a single <em>feature</em> or <em>attribute</em>.
This means that each example (row) is a vector of features (columns). This
definition is quite flexible and works for many networks problems at different
scales. For example, each example might correspond to an IPFIX flow record,
with features corresponding to the number of packets in the flow, the start
and end times of the flow, the source and destination IP addresses, etc. In a
different problem, each example might correspond to a single user, with
features representing average throughput and latency to their home router in
1-hour intervals. This flexibility is a boon, because it allows developers of
machine learning algorithms to work with a consistent mathematical
formulation—a matrix (or table) of examples and features. In this book, we
will generally refer to a matrix of training examples as <strong>X</strong>. Sometimes in
code examples we will also use <strong>features</strong> to refer to the matrix of training
examples.</p>
<table class="colwidths-given docutils align-default" id="id3">
<caption><span class="caption-number">Table 1. </span><span class="caption-text">Training Examples</span><a class="headerlink" href="#id3" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Feature 1: Number of packets</p></th>
<th class="head"><p>Feature 2: Start time</p></th>
<th class="head"><p>Feature 3: End time</p></th>
<th class="head"><p>Feature 4: Source IP</p></th>
<th class="head"><p>Feature 5: Destination IP</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>NNNN</p></td>
<td><p>NNNN</p></td>
<td><p>NNNN</p></td>
<td><p>NNNN</p></td>
<td><p>NNNN</p></td>
</tr>
<tr class="row-odd"><td><p>NNNN</p></td>
<td><p>NNNN</p></td>
<td><p>NNNN</p></td>
<td><p>NNNN</p></td>
<td><p>NNNN</p></td>
</tr>
</tbody>
</table>
<p>In addition to training examples, creating a supervised machine learning model also requires <em>training labels</em>. For single-label tasks, you will have one label for every example in your data set. So if you have a matrix of training examples with 20 rows, you will need to have 20 labels (one for each example). The training labels are therefore usually represented as a vector with the same number of elements as the number of training examples. For instance, each training label for a security task might be a 1 if the corresponding example is malicious or a 0 if the corresponding example is benign.</p>
<table class="colwidths-given docutils align-default" id="id4">
<caption><span class="caption-number">Table 2. </span><span class="caption-text">Training Labels for Single-Label Task</span><a class="headerlink" href="#id4" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 100%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Label 1: Is malicious</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>NNNN</p></td>
</tr>
<tr class="row-odd"><td><p>NNNN</p></td>
</tr>
</tbody>
</table>
<p>For multi-label tasks, you can have multiple label vectors or a combined label matrix. For example, you might want to predict users’ responses to a quality of experience questionairre. For each question on the questionairre, you would need a label corresponding to each user.</p>
<table class="colwidths-given docutils align-default" id="id5">
<caption><span class="caption-number">Table 3. </span><span class="caption-text">Training Labels for Multi-Label Task</span><a class="headerlink" href="#id5" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Label 1: —</p></th>
<th class="head"><p>Label 1: —</p></th>
<th class="head"><p>Label 1: —</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>NNNN</p></td>
<td><p>NNNN</p></td>
<td><p>NNNN</p></td>
</tr>
<tr class="row-odd"><td><p>NNNN</p></td>
<td><p>NNNN</p></td>
<td><p>NNNN</p></td>
</tr>
</tbody>
</table>
<p>For convenience, the vector (or matrix, for multi-label tasks) of training labels are usually given the variable name y. The first dimension of the training labels is the same cardinality as the first dimension of the training examples.</p>
</div>
<div class="section" id="types-of-labels">
<h3>Types of Labels<a class="headerlink" href="#types-of-labels" title="Permalink to this headline">¶</a></h3>
<p>The format of the training labels distinguishes <em>regression</em> problems from <em>classification</em> problems. Regression problems involve labels that are continuous numbers. Classification problems involve discrete labels (e.g. integers or strings). Classification tasks can be further categorized into <em>binary classification</em> and <em>multi-class classification</em> by the number of distinct labels.
While some supervised machine learning algorithms can be trained to perform either classification or regression tasks, others only work for one or the other. When choosing a machine learning algorithm, you should pay attention to whether you are attempting a classification or regression task. Our description of common machine learning models in the next chapter clearly states whether each algorithm is applicable to classification, regression, or both.</p>
<p><strong>Regression:</strong> Training labels for regression problems are simply encoded as real-value numbers, usually in a floating point type. Regression may be familiar if you have experience fitting lines or curves to existing data in order to interpolate or extrapolate missing values using statistics techniques. Regression problems can also be performed with integer labels, but be aware that most off-the-shelf regression algorithms will return predictions as floating point types. If you want the predictions as integers, you will need to round or truncate them after the fact.</p>
<p><strong>Binary Classification:</strong> Training labels for binary classification problems are encoded as 1s and 0s. The developer decides which class corresponds to 1 and which class corresponds to 0, making sure to keep this consistent throughout the ML training and deployment process.</p>
<p><strong>Multi-class Classification:</strong> Training labels for multi-class classification problems are encoded as discrete values, with at least three different values represented in the vector of training labels y. Note that <em>multi-class classification</em> (more than two discrete classes) is distinct from <em>multi-label classification</em> (multiple independent classification tasks, each with their own set of discrete classes).
There are multiple options for encoding training labels in multi-class classification problems. Since most machine learning algorithms require training labels as numeric types (e.g. int, float, double), string class names usually need to be encoded before they can be used for ML training (decision trees are a notable exception to this requirement). For example, if you have labels indicating users’ quality of experience on a “poor”, “acceptable”, “excellent” scale, you will need to convert each response into a number in order to train an algorithm to predict the QoE of new users. One simple way to perform this encoding is to assign each class to a unique integer (<em>ordinal encoding</em>). In the example above, this mapping could convert “poor” –&gt; 0, “acceptable” –&gt; 1, and “excellent” –&gt; 2. The fully encoded training labels would then be a vector of integers with one element for each training example.
An alternative way to encode multi-class training labels is to perform <em>one-hot encoding</em>, which represents each class as a binary vector. If there are N possible classes, the one-hot encoding of the ith class will be an N-element binary vector with a 1 in element N and a 0 in all other elements. In the example above, this mapping could convert “poor” –&gt; [1,0,0], “acceptable” –&gt; [0,1,0], and “excellent” –&gt; [0,0,1].</p>
<p>While it is useful to understand these subcategories of supervised learning, it is also wise not to assign excessive importance to their distinctions. Many machine learning algorithms are effective for either classification or regression tasks with minimial modifications. Additionally, it is often possible to translate a classification task into a regression task, or vice versa. Consider the example of predicting improvements in last-mile network throughput for different possible infrastructure upgrades. While it is reasonable to express throughput as a real-valued number in Mbps, you may not care about small distinctions of a few bits per second. Instead, you could reframe the problem as a classification task and categorize potential updates into a few quality of experience classes.</p>
</div>
<div class="section" id="train-test-sets">
<h3>Train &amp; Test Sets<a class="headerlink" href="#train-test-sets" title="Permalink to this headline">¶</a></h3>
<p>The goal of supervised learning is to train a model that takes observations (examples) and predicts labels for these examples that are as close as possible to the actual labels. For instance, a model might take IPFIX records and predict quality of experience labels, with the goal that the predicted labels are as accurate as possible to the actual quality experienced by the customer.</p>
<p>However, if you don’t know the correct labels for new observations, how do you measure whether the model is succeeding? This is an important question and one that must be considered at the beginning of the ML process. Imagine that you’ve trained a machine learning algorithm with training examples and training labels, then you deploy it in the wild. The algorithm starts seeing new data and producing predictions for that data. How can you evaluate whether it is working?</p>
<p>The way to solve this problem is to test the performance of the trained algorithm on additional data that it has never seen, but for which you already know the correct labels. This requires that you train the algorithm using only a portion of the entire labeled dataset (the <em>training set</em>) and withold the rest of the labeled data (the <em>test set</em>) for testing how well the model generalizes to new information.</p>
<p>The examples that you reserve for the test set should be randomly selected to avoid bias in most cases. However, some types of data require more care when choosing test set examples.
If you have data for which the order of the examples is particularly important, then you should select a random contiguous subset of that data as your test set. That maintains the internal structure that is relevant for understanding the test examples. This is particularly important for chronological data (e.g. packets from a network flow). If you do not select a contiguous subset, you may end up with a nonsensical test set with examples from one day followed by examples from a year later followed by examples from a few months after that.</p>
<p>This brings us to the golden rule of supervised machine learning: <strong>never train on the test set!</strong> Nothing in the final model or data pre-processing pipeline should be influenced by the examples in the test set. This includes model parameters and hyperparameters (more below). The test set is only for testing and reporting your final performance. This allows the performance evaluation from the test set to be as close of an approximation as possible to the <em>generalization error</em> of the model, or how well the model would perform on new data that is independent but identically distributed (i.i.d.) to the test set. For example, when you report that your model performed with 95% accuracy on a test set, this means that you also expect 95% accuracy on new i.i.d. data in the wild. This caveat about i.i.d. matters, because if your training data and/or test set is not representative of the real-world data seen by the model, the model is unlikely to perform as well on the real-world data (imagine a trained Chess player suddenly being asked to play in a Go tournament).</p>
<p>The best way to avoid breaking the golden rule is to program your training pipeline to ensure that once you separate the test set from the training set, the test set is never used until the final test of the final version of the model. This seems straightforward, but there are many ways to break this rule.</p>
<p>A common mistake involves performing a pre-processing transformation that is based on all labeled data you’ve collected, including the test set. A common culprit is <em>standardization</em>, which involves normalizing each feature (column) of your dataset such that variance matches a prespecified distribution with mean of zero. This is a very common pre-processing step, but if you compute standardization factors based on the combined training and test set, your test set accuracy will not be a true measure of the generalization error. The correct approach is to compute standardization factors based only on the training set and then use these factors to standardize the test set.</p>
<p>Another common mistake involves modifying some hyperparameter of the model after computing the test error. Imagine you train your algorithm on the training set, you test it on the test set, and then you think, “Wow, my test accuracy was terrible. Let me go back and tweak some of these parameters and see if I can improve it.” That’s an admirable goal, but poor machine learning practice, because then you have tuned your model to your particular test set, and its performance on the test set is no longer a good estimate of its error on new data.</p>
</div>
<div class="section" id="validation-sets-and-cross-validation">
<h3>Validation Sets and Cross-Validation<a class="headerlink" href="#validation-sets-and-cross-validation" title="Permalink to this headline">¶</a></h3>
<p>The correct way to test generalization error and continue to iterate, while still being able to test the final generalization error of the completed model, involves dividing your labeled data into three parts: 1) a training set, which you use to train the algorithm, 2) A test set, that is used to test the generalization error of the <strong>final</strong> model, and 3) the <em>validation set</em>, which is used to test the generalization error of intermediate versions of the model as you tune the hyperparameters. This way you can check the generalization performance of the model while still reserving some completely new data for reporting the final performance on the test set.</p>
<p>If your final model performs well accross the training, validation, and test sets, you can be quite confident that it will perform well on other new data as well. Alternatively, your final model might perform well on the training and validation sets but poorly on the test set. That would indicate that the model is overly tuned to the specifities of the training and validation sets, causing it to generalize poorly to new data. This phenomenon is called <em>overfitting</em> and is a pervasive problem for supervised machine learning. Overfitting has inspired many techniques to avoid and mitigate, several of which we describe below.</p>
<p>While dividing your labeled data into training, validation, and test sets works well conceptually, it has a practical drawback. The amount of data used to actually train the model (the training set) is significantly reduced. If you apply a 60% training, 20% validation, 20% test split, that leaves only 60% of your data for actual training. This can reduce performance, because supervised ML models generally perform better with more training data.</p>
<p>The solution to this is to use a process called <em>cross validation</em>, which allows you to combine the training and validation sets through a series of sequential model trainings called <em>folds</em>. In each fold, you pull out a subset of the training data for validation and train the model on the rest. You repeat this process with non-overlapping subsets for each fold, such that every training example has been in a validation fold once.  In a 5-fold cross-validation, each fold would use 20% of the non-test data as a validation set and the remaining 80% for training. This would result in 5 different validation performance measurements (one for each folds) that you can average together for the <em>average cross-validation performance</em>.</p>
<p>Cross-validation provides a more robust measure of generalization performance than fixed training and validation sets. It is so common that supervised machine learning results are often reported in terms of the average performance on an N-fold cross-validation.</p>
<p>So how many folds is enough? Generally, the more folds, the better estimation of model performance. In the extreme case, called <em>leave-one-out cross-validation</em>, each fold uses a validation set with just one example, allowing the rest to be used for training. Each fold tests whether the model correctly predicts this one example when trained on all other examples. This provides the best estimate of generalization performance, because you’re training with the most data possible. If you have labeled 100 examples in your non-test set, leave-one-out cross-validation involves performing 100 folds and averaging the accuracy across accross all 100. You can then repeat this process while tuning model hyperparameters to check the effects on generalization accuracy.</p>
<p>Unfortunately, leave-one-out cross-validation has its own drawbacks. Every cross-validation fold involves training a model, and if the training process is resource intensive, leave-one-out cross-validation can take a long time. Some models, especially neural network models, take a long time to train, so doing more than a few folds becomes impractical.
For such models, a smaller number of folds (often 5 or 10) are used in practice.</p>
</div>
</div>
<div class="section" id="model-training">
<h2>Model Training<a class="headerlink" href="#model-training" title="Permalink to this headline">¶</a></h2>
<p>Training a supervised machine learning model involves adjusting the model’s parameters to improve its ability to predict the training labels from the training examples. This adjustment process takes place automatically, typically using an iterative algorithm (such as gradient descent) to gradually optimize the model’s parameters.
If the training process works correctly, it will produce a model that performs as well as possible on the training set.
Whether or not this trained model will generalize to new data must be tested using a validation and/or test set.</p>
<p>It is important to note the difference between model <em>parameters</em> and <em>hyperparameters</em>.
Parameters are the elements of the model that the training algorithm automatically modifies when attempting to improve the model’s performance on the training set.
Hyperparameters are “meta”-parameters that can be tuned manually or automatically, depending on the degree to which the model creation process is automated.</p>
<p>Model training is a minimization process. Each training step attempts to modify the model’s parameters to reduce the difference, or <em>error</em>, between the labels predicted by the model and the actual training labels.
You can choose which error function you want the training process to minimize.
Square error, square difference, or absolute error are good for regression problems, while others, such as categorical crossentropy, are better for classification problems.
We’ll define and see equations for once we start looking at training of specific model types.</p>
<div class="section" id="error-minimization-and-gradient-descent">
<h3>Error Minimization and Gradient Descent<a class="headerlink" href="#error-minimization-and-gradient-descent" title="Permalink to this headline">¶</a></h3>
<p>A few combinations of models and error functions have closed form solutions for minimization, allowing you to directly solve for the optimal model parameters.
However, most models and error function combinations used in practice either don’t have a closed form solution or the computation cost of the closed form solution is infeasible.
The solution is to apply an iterative process, usually a version of <em>gradient descent</em>, starting with arbitrarily chosen initial parameter values and incrementally improving them until values close to the optimal are found.</p>
<p>Gradient descent is a mathmatical representation of tha idea that if you can’t see the solution to a problem, just repeatedly take one step in the right direction.
By computing or approximating the gradient of the error of a model with respect to the model’s parameters, it is possible to update the parameters “down” the gradient and improve the model’s performance.
Computing or approximating a gradient is often easier than directly solving for a minimum, making gradient descent a feasible way to train many types of complex models.</p>
<p>The size of “step” that each parameter takes during each gradient descent iteration can be turned using a hyperparameter called the <em>learning rate</em>. A high learning rate will update the parmaetrs a relatively large amount with every step, while  a low learning rate will update your parmaters a small amount with every step.
There exist many algorithms for dynamically tuning the learning rate during the training process to improve training speed while avoiding some common pitfalls of gradient descent (more on these below).
In practice, you will likely use one of these existing algorithms rather than controlling the learning rate manually.</p>
<p>The gradient descent process continues until the model parameter stop changing between iteration (or at least until they change less than a predetermined small value).
When this happens, the parameters define a model that is in a local minimum of the error function, which is hopefully close to the optimum.</p>
<p>There are a few variations to gradient descent that gain even more computational efficiency.
The first, <em>batch gradient descent</em> uses the entire training data set to compute the gradient at each iteration. This provides an accurate gradient value, but may be computationally expensive. An alternative approach, <em>stochastic gradient descent</em> uses only a single example from the training set to estimate the gradient at each iteration. After N iterations, all N examples in the training set will have used for a gradient estimate once. Stochastic gradient descent allows for faster parameter update iterations than batch gradient descent, but the estimated gradients will be less accurate. Whether or not this results in a faster overall training time depends on the specifics of the model and the training set.
<em>Mini-batch gradient descent</em> is a third option that allows for a tradeoff between the extremes of batch and stochastic gradient descent. In mini-batch gradient descent, a hyperparameter called the <em>batch size</em> determines how many training examples are used to estimate the gradient during each iteration.
This provides a great deal of flexibility, but introduces the need for one more term frequently used when discussing model training: the <em>epoch</em>. One <em>epoch</em> is one sequence of gradient descent iterations for which each example in the training set is used to estimate exactly one gradient. In batch gradient descent, iterations are equivalent to epochs, because all training data is used for the gradient calculation. In stochastic gradient descent, each epoch consists of N iterations for a training set with N examples. The number of iterations per epoch in mini-batch gradient descent depends on the batch size. With N training examples and a batch size of B, there will be ceiling(N/B) iterations per epoch.</p>
<p>Gradient descent is the heart of machine learning, but it’s not without problems.
Many ML models, especially deep learing models, have gradients that are non-convex, so rather than converging on the global minimum, gradient descent gets stuck in a local minimum or plateau and is unable to make further progress. Getting stuck in a plateau is known as the <em>vanishing gradient problem</em> and especially plagues deep learning models.
Fortunately, algorithms that dynamically modify the learning rate during training can help avoid these situations.
For example, the <em>simulated annealing</em> algorithm starts with a large learning rate and graduate decreases the rate throughout the learning process, making it less likely that the process gets stuck far away from the global minimum.</p>
</div>
<div class="section" id="overfitting">
<h3>Overfitting<a class="headerlink" href="#overfitting" title="Permalink to this headline">¶</a></h3>
<p><em>Overfitting</em> is a common problem for many supervised machine learning models. An “overfit” model has been overly tuned to idiosyncracies of the training set and doesn’t generalize well to new data.</p>
<p>It’s useful to be able to detect when your model is overfitting. One way is to compare the model’s training error and validation error when trained on increasing fractions of the training set or for increasing number of gradient descent steps. As long as the training error is close to the validation error, you can be fairly confident that the mdoel isn’t overfitting (and is likely <a href="#id1"><span class="problematic" id="id2">*</span></a>under*fitting, requiring more training data or training iterations to improve its performance). If the validation error is significantly worse than the training error, the model is likely overfitting.</p>
<p>There are several ways to deal with overfitting.
The best approach is to collect more training data such that your training set is more representative of any new data the model will see in the wild. Another approach, <em>early stopping</em> involves stopping the training process when the training error and validation error start to diverge. <em>Regularization</em> places a penalty on parameter complexity during the training process, helping to prevent the model from becoming overly tuned to the training examples.</p>
</div>
</div>
<div class="section" id="model-evaluation">
<h2>Model Evaluation<a class="headerlink" href="#model-evaluation" title="Permalink to this headline">¶</a></h2>
<p>Before we can dive into learning about machine learning algorithms, we need to define a few more concepts to help us determine whether these algorithms are meeting their goals. First, we need to define what we mean by “close to the real label.”  This is another way of saying “how do we measure the success of the model?”</p>
<p>There are many performance metric functions. In general, they compare predictions that your model produces to the correct labels and prodice a real number indicating the performance of the model.</p>
<div class="section" id="supervised-metrics">
<h3>Supervised Metrics<a class="headerlink" href="#supervised-metrics" title="Permalink to this headline">¶</a></h3>
<p>For regression problems, you’ll often use an error function like mean squared error or mean absolute error. In this example, we have some data points, we’ve plotted a linear regression line, we could see the errors of the points off of the line. And you could report the average of those errors are the average of those squared errors as the performance of your model. The training process will attempt to minimize this error to produce a regression model that produces predictions which are as close as possible to all data points on average.</p>
<p>For classification, we can’t use mean or absolute errors directly, because the class labels are not real-valued and may not be ordinal. For example, [EX]</p>
<p><em>Accuracy</em> is popular performance metric for classification problem because it is intuitive. The accuracy of a model is the ratio of correct predictions to all predictions. For example [EX]. For some problems, this works pretty well. But for others just reporting accuracy is deceptive.</p>
<p>Imagine you’re training a machine learning algorithm to take in packets and classify them as “malicious” or “benign”. If you just use the naive algorithm “always predict benign”, the accuracy will be high accuracy, because most most packets aren’t malicious.  So if you have a problem for which the actual classes aren’t evenly balanced among all of the possible classes, your accuracy score isn’t going to be as intuitive as it should be. For example, you shouldn’t necessarily assume that anything over 50% accuracy is great. 50% only a meaningful metric if you are doing binary classification where you’re expecting even numbers of both classes examples. In multi-class problems or tasks where certain classes are rare, accuracy isn’t a great way to report your results.</p>
<p><em>Precision &amp; Recall</em>
A better way to report classification performance accuracy for multi-class and non-balanced clases is with a precision or recall score. These scores are best understood in the context of binary classification tasks. The precision score is the percentage of the elements that you correctly predicted were in a class of interest. A high precision score is good. You want most of the things that you predicted to be in that particular class to actually be in that class. The recall score is the number of things actually in a class of interest that you predicted were in that class. Prediction and recall are often two sides of a coin. You will see that if you make parameter or hyperparameter changes to improve precision, you may end up reducing recall (and vice versa). (ADD TRUE POSITIVES ETC.)</p>
<p>A high precision algorithm may not make many predictions, but when it does, it’s right. A high recall algorithm may make a lot of predictions hitting  most of the examples in the class of interest, but also a bunch of exampless ones that we didn’t care about. In different real world situations, you might want to tune your algorithm to have a higher precision or a higher recall, depending on your application. [EX]</p>
<p>For example, consider a machine learning system that is detecting malware in a mission critical system. It may not matter if there are some false alarms (low precision), but you want to make sure the system doesn’t miss any actual malware events (high recall). In comparison, consider spam detection. You may be okay with your spam detector allowing some spam through to the inbox (low recall), but you don’t want the detector to accidentally classify any non-spam messages as spam (high precision).</p>
<p>Since you can tune many algorithms to trade off precision and recall, you’ll often plot a precision/recall curve that shows how the tuning of some parameter affects precision and the recall. [EX PLOT] An ideal algorithm will have a curve that looks like [EX]</p>
<p>Now, if you aren’t sure where to pick on this curve, you’ll often choose the like the Pareto optimal setting where any trade off that you make in one direction is outweighed by the trade off that you’d make in another direction. And if you’re familiar with Pareto optimality from economics, it’s exactly the same idea.</p>
<p>However, sometimes you want to report a single number that says how well your algorithm performs instead of both precision and recall scores. The standard approach is to report an F1 score, which is the harmonic mean of precision and recall. The harmonic mean causes the F1 score to weigh low values higher. This means thata a high precision won’t offset a partularly low recall or vice versa.</p>
<p><em>Receiver Operating Characteristic</em>
The receiver operating characteristic (ROC) is another performance measure that uses the <em>true positive rate</em> and the <em>false positive rate</em>.
The true positive rate is exactly the same as recall, it’s just another name for the same concept. The false positive rate is a little different from precision. The false positive rate is the fraction of examples that we labeled as the class of interest but we were incorrect about. TPR and FPR are often used in terms of epidemiology.</p>
<p>Just like for precision recall, you can tune hyper parameters to affect the true positive rate and false positive rate. You can plot them on a curve  called an ROC curve. As you change the parameter to increase the true positive rate, you may end up with more false positives, too. If you want a single metric to use for the algorithm performance that incorporates the fact that you can tune these parameters, you can use the area under the ROC curve. Since your algorithm would ideally be able to achieve a high true positive rate with a very low false positive rate, you’d have this curve [EX], which is right along the edge of the graph, the ideal area under the curve is 1.</p>
<p><em>Confusion matrices</em>
Confusion matrices go beyond a single number or a scalar metric for classification performance and instead provide details about how your model is making mistakes. A confusion matrix plots the frequency or the number of examples that were given a particular label versus the correct label. If your model is doing perfectly, you will end up with a diagonal matrix where all of the examples are in the correct class. If your model is sometimes making mistakes, there will be numbers outside of this diagonal. You can then investigate these examples to see why they might be more difficult for the model and focus on these cases to improve your performance.
Confusion matrics are useful for debugging and iterative improvements.</p>
</div>
<div class="section" id="unsupervised-metrics">
<h3>Unsupervised Metrics<a class="headerlink" href="#unsupervised-metrics" title="Permalink to this headline">¶</a></h3>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="supervised.html" class="btn btn-neutral float-right" title="Chapter 5: Supervised Learning Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="measurement.html" class="btn btn-neutral float-left" title="Chapter 3: Network Measurement" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>